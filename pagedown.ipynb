{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mebigti/mqra/blob/main/pagedown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b067b598-9f68-4624-ac06-8d57de410c00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b067b598-9f68-4624-ac06-8d57de410c00",
        "outputId": "16a37b64-b55e-4672-8a91-e3f62666cc62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded resources to folder: downloaded_website\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "def download_file(url, folder):\n",
        "    # Ensure the folder structure exists\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    local_filename = os.path.join(folder, os.path.basename(urlparse(url).path))\n",
        "    with requests.get(url, stream=True) as response:\n",
        "        if response.status_code == 200:\n",
        "            with open(local_filename, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "    return local_filename\n",
        "\n",
        "def download_website_resources(url, download_folder):\n",
        "    # Create download folder if it doesn't exist\n",
        "    if not os.path.exists(download_folder):\n",
        "        os.makedirs(download_folder)\n",
        "\n",
        "    # Download the main HTML page\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve the page: {response.status_code}\")\n",
        "        return\n",
        "\n",
        "    # Parse the HTML using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    html_file_path = os.path.join(download_folder, 'index.html')\n",
        "    with open(html_file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(soup.prettify())\n",
        "\n",
        "    # Download CSS and JavaScript files\n",
        "    tags = {'link': 'href', 'script': 'src'}\n",
        "    for tag, attribute in tags.items():\n",
        "        for resource in soup.find_all(tag):\n",
        "            resource_url = resource.get(attribute)\n",
        "            if resource_url:\n",
        "                resource_url = urljoin(url, resource_url)\n",
        "                folder_name = 'css' if tag == 'link' else 'js'\n",
        "                resource_folder = os.path.join(download_folder, folder_name)\n",
        "                if tag == 'link' and resource.get('rel') == ['stylesheet']:\n",
        "                    download_file(resource_url, resource_folder)\n",
        "                elif tag == 'script':\n",
        "                    download_file(resource_url, resource_folder)\n",
        "\n",
        "    # Download images\n",
        "    for img in soup.find_all('img'):\n",
        "        img_url = img.get('src')\n",
        "        if img_url:\n",
        "            img_url = urljoin(url, img_url)\n",
        "            img_folder = os.path.join(download_folder, 'images')\n",
        "            download_file(img_url, img_folder)\n",
        "\n",
        "    # Download other resources (e.g., layers)\n",
        "    for script in soup.find_all('script'):\n",
        "        script_url = script.get('src')\n",
        "        if script_url and 'layers' in script_url:\n",
        "            script_url = urljoin(url, script_url)\n",
        "            layers_folder = os.path.join(download_folder, 'layers')\n",
        "            download_file(script_url, layers_folder)\n",
        "\n",
        "    print(f\"Downloaded resources to folder: {download_folder}\")\n",
        "\n",
        "# Example usage\n",
        "website_url = 'https://0404.go.kr/new_osm/index_x.jsp?a1=15571139.906030&a2=4476152.376380&a3=1&a4='\n",
        "download_folder = 'downloaded_website'\n",
        "download_website_resources(website_url, download_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc1cdfc8-3ce9-49a7-84cf-a08eac52a2be",
      "metadata": {
        "id": "dc1cdfc8-3ce9-49a7-84cf-a08eac52a2be"
      },
      "source": [
        "인덱스 파일 하나만 저장\n",
        "개발자 도구를 캡쳐하여 GPT에게 어떻게 폴더 구조 그대로 내 컴퓨터에 저장하는지 코드를 작성해달라고 요청함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d43eb660-052b-4be2-83d0-ea1731e96dee",
      "metadata": {
        "id": "d43eb660-052b-4be2-83d0-ea1731e96dee",
        "outputId": "650521df-a397-411c-a196-db92907768ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded resources to folder: downloaded_website\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "def download_file(url, folder):\n",
        "    # Ensure the folder structure exists\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    local_filename = os.path.join(folder, os.path.basename(urlparse(url).path))\n",
        "    with requests.get(url, stream=True) as response:\n",
        "        if response.status_code == 200:\n",
        "            with open(local_filename, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "    return local_filename\n",
        "\n",
        "def download_website_resources(url, download_folder):\n",
        "    # Create download folder if it doesn't exist\n",
        "    if not os.path.exists(download_folder):\n",
        "        os.makedirs(download_folder)\n",
        "\n",
        "    # Download the main HTML page\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve the page: {response.status_code}\")\n",
        "        return\n",
        "\n",
        "    # Parse the HTML using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    html_file_path = os.path.join(download_folder, 'index.html')\n",
        "    with open(html_file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(soup.prettify())\n",
        "\n",
        "    # Download CSS and JavaScript files\n",
        "    tags = {'link': 'href', 'script': 'src'}\n",
        "    for tag, attribute in tags.items():\n",
        "        for resource in soup.find_all(tag):\n",
        "            resource_url = resource.get(attribute)\n",
        "            if resource_url:\n",
        "                resource_url = urljoin(url, resource_url)\n",
        "                folder_name = 'css' if tag == 'link' else 'js'\n",
        "                resource_folder = os.path.join(download_folder, folder_name)\n",
        "                if tag == 'link' and resource.get('rel') == ['stylesheet']:\n",
        "                    download_file(resource_url, resource_folder)\n",
        "                elif tag == 'script':\n",
        "                    download_file(resource_url, resource_folder)\n",
        "\n",
        "    # Download images\n",
        "    for img in soup.find_all('img'):\n",
        "        img_url = img.get('src')\n",
        "        if img_url:\n",
        "            img_url = urljoin(url, img_url)\n",
        "            img_folder = os.path.join(download_folder, 'images')\n",
        "            download_file(img_url, img_folder)\n",
        "\n",
        "    # Download other resources (e.g., layers)\n",
        "    for script in soup.find_all('script'):\n",
        "        script_url = script.get('src')\n",
        "        if script_url and 'layers' in script_url:\n",
        "            script_url = urljoin(url, script_url)\n",
        "            layers_folder = os.path.join(download_folder, 'layers')\n",
        "            download_file(script_url, layers_folder)\n",
        "\n",
        "    print(f\"Downloaded resources to folder: {download_folder}\")\n",
        "\n",
        "# Example usage\n",
        "website_url = 'https://0404.go.kr/new_osm/'\n",
        "download_folder = 'downloaded_website'\n",
        "download_website_resources(website_url, download_folder)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}